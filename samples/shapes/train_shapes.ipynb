{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenmaoyuan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ad670bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a6658da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a66316400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a665aff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/shenmaoyuan/proj/Mask_RCNN/logs/shapes20180411T1732/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.7598 - rpn_class_loss: 0.0361 - rpn_bbox_loss: 0.8420 - mrcnn_class_loss: 0.5365 - mrcnn_bbox_loss: 0.6907 - mrcnn_mask_loss: 0.6545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2330: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 13s 131ms/step - loss: 2.7490 - rpn_class_loss: 0.0359 - rpn_bbox_loss: 0.8384 - mrcnn_class_loss: 0.5352 - mrcnn_bbox_loss: 0.6871 - mrcnn_mask_loss: 0.6524 - val_loss: 1.4210 - val_rpn_class_loss: 0.0169 - val_rpn_bbox_loss: 0.3764 - val_mrcnn_class_loss: 0.2463 - val_mrcnn_bbox_loss: 0.3845 - val_mrcnn_mask_loss: 0.3969\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.4074 - rpn_class_loss: 0.0194 - rpn_bbox_loss: 0.5154 - mrcnn_class_loss: 0.2786 - mrcnn_bbox_loss: 0.3128 - mrcnn_mask_loss: 0.2812 - val_loss: 1.0429 - val_rpn_class_loss: 0.0148 - val_rpn_bbox_loss: 0.3813 - val_mrcnn_class_loss: 0.1507 - val_mrcnn_bbox_loss: 0.2738 - val_mrcnn_mask_loss: 0.2222\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.0815 - rpn_class_loss: 0.0146 - rpn_bbox_loss: 0.4115 - mrcnn_class_loss: 0.2324 - mrcnn_bbox_loss: 0.2397 - mrcnn_mask_loss: 0.1832 - val_loss: 1.0480 - val_rpn_class_loss: 0.0117 - val_rpn_bbox_loss: 0.5487 - val_mrcnn_class_loss: 0.1969 - val_mrcnn_bbox_loss: 0.1579 - val_mrcnn_mask_loss: 0.1329\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.0283 - rpn_class_loss: 0.0174 - rpn_bbox_loss: 0.4671 - mrcnn_class_loss: 0.1898 - mrcnn_bbox_loss: 0.1980 - mrcnn_mask_loss: 0.1559 - val_loss: 1.0454 - val_rpn_class_loss: 0.0094 - val_rpn_bbox_loss: 0.4145 - val_mrcnn_class_loss: 0.1630 - val_mrcnn_bbox_loss: 0.2638 - val_mrcnn_mask_loss: 0.1947\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.9130 - rpn_class_loss: 0.0154 - rpn_bbox_loss: 0.4152 - mrcnn_class_loss: 0.1579 - mrcnn_bbox_loss: 0.1889 - mrcnn_mask_loss: 0.1356 - val_loss: 0.9748 - val_rpn_class_loss: 0.0140 - val_rpn_bbox_loss: 0.4833 - val_mrcnn_class_loss: 0.1426 - val_mrcnn_bbox_loss: 0.1932 - val_mrcnn_mask_loss: 0.1417\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=5, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 5. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /home/shenmaoyuan/proj/Mask_RCNN/logs/shapes20180411T1732/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7692 - rpn_class_loss: 0.0142 - rpn_bbox_loss: 0.3700 - mrcnn_class_loss: 0.1253 - mrcnn_bbox_loss: 0.1285 - mrcnn_mask_loss: 0.1312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenmaoyuan/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2330: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 22s 225ms/step - loss: 0.7716 - rpn_class_loss: 0.0144 - rpn_bbox_loss: 0.3708 - mrcnn_class_loss: 0.1262 - mrcnn_bbox_loss: 0.1288 - mrcnn_mask_loss: 0.1315 - val_loss: 0.8499 - val_rpn_class_loss: 0.0150 - val_rpn_bbox_loss: 0.4182 - val_mrcnn_class_loss: 0.1755 - val_mrcnn_bbox_loss: 0.1324 - val_mrcnn_mask_loss: 0.1089\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 0.6902 - rpn_class_loss: 0.0150 - rpn_bbox_loss: 0.3917 - mrcnn_class_loss: 0.1028 - mrcnn_bbox_loss: 0.0930 - mrcnn_mask_loss: 0.0877 - val_loss: 0.5976 - val_rpn_class_loss: 0.0143 - val_rpn_bbox_loss: 0.3315 - val_mrcnn_class_loss: 0.0567 - val_mrcnn_bbox_loss: 0.0999 - val_mrcnn_mask_loss: 0.0952\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 20s 200ms/step - loss: 0.7026 - rpn_class_loss: 0.0154 - rpn_bbox_loss: 0.3700 - mrcnn_class_loss: 0.1051 - mrcnn_bbox_loss: 0.1022 - mrcnn_mask_loss: 0.1099 - val_loss: 0.7105 - val_rpn_class_loss: 0.0149 - val_rpn_bbox_loss: 0.4457 - val_mrcnn_class_loss: 0.0512 - val_mrcnn_bbox_loss: 0.1151 - val_mrcnn_mask_loss: 0.0835\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 0.6433 - rpn_class_loss: 0.0162 - rpn_bbox_loss: 0.3501 - mrcnn_class_loss: 0.0902 - mrcnn_bbox_loss: 0.0892 - mrcnn_mask_loss: 0.0976 - val_loss: 0.6170 - val_rpn_class_loss: 0.0154 - val_rpn_bbox_loss: 0.2946 - val_mrcnn_class_loss: 0.1073 - val_mrcnn_bbox_loss: 0.0918 - val_mrcnn_mask_loss: 0.1080\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 0.6834 - rpn_class_loss: 0.0154 - rpn_bbox_loss: 0.3639 - mrcnn_class_loss: 0.1090 - mrcnn_bbox_loss: 0.0921 - mrcnn_mask_loss: 0.1030 - val_loss: 0.9488 - val_rpn_class_loss: 0.0122 - val_rpn_bbox_loss: 0.3175 - val_mrcnn_class_loss: 0.1901 - val_mrcnn_bbox_loss: 0.1577 - val_mrcnn_mask_loss: 0.2713\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=10, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/shenmaoyuan/proj/Mask_RCNN/logs/shapes20180411T1732/mask_rcnn_shapes_0010.h5\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   45.00000  max:  155.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\n",
      "gt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\n",
      "gt_bbox                  shape: (1, 4)                min:   28.00000  max:  128.00000  int32\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a66757860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:   45.00000  max:  155.00000  uint8\n",
      "molded_images            shape: (1, 128, 128, 3)      min:  -71.80000  max:   38.20000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int64\n",
      "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79ab324c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
